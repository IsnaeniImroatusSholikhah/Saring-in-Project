{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JpLmjQva7Ee"
      },
      "source": [
        "Ngrok Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfsygGnA6dxC",
        "outputId": "47fb4363-1e34-4657-b60e-41dcb3ce1518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXxP2zD_FvaL",
        "outputId": "ccfd157d-4612-4889-fd4f-c8f9f4d54e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-13 12:00:17--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 54.237.133.81, 18.205.222.128, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-06-13 12:00:17 (182 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "!./ngrok authtoken +++ PUT NGROK KEY HERE +++\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiLKlojlbEfK"
      },
      "source": [
        "Install flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPt8Ft98FSJ8",
        "outputId": "452e1c33-de03-4cf5-bdb1-55bf158108a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.27.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.3.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvOuoJEcbNqo"
      },
      "source": [
        "Scraping Detik.com function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOXPBOtJHDoL"
      },
      "outputs": [],
      "source": [
        "import requests as req\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime\n",
        "import csv\n",
        "hades = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
        "\n",
        "def scrape_detik(hal,query):\n",
        "    global hades\n",
        "    a = 1\n",
        "    toReturn = []\n",
        "    for page in range(1,hal):\n",
        "        url = f'https://www.detik.com/search/searchnews?query={query}&sortby=time&page={page}'\n",
        "        ge = req.get(url,hades).text\n",
        "        sop = bs(ge,'lxml')\n",
        "        li = sop.find('div',class_='list media_rows list-berita')\n",
        "        lin = li.find_all('article')\n",
        "        for x in lin:\n",
        "            link = x.find('a')['href']\n",
        "            date = x.find('a').find('span',class_='date').text.replace('WIB','').replace('detikNews','').split(',')[1]\n",
        "            headline = x.find('a').find('h2').text\n",
        "            ge_ = req.get(link,hades).text\n",
        "            sop_ = bs(ge_,'lxml')\n",
        "            content = sop_.find_all('div',class_='detail__body-text itp_bodycontent')\n",
        "            for x in content:\n",
        "                x = x.find_all('p')\n",
        "                y  = [y.text for y in x ]\n",
        "                content_ = ''.join(y).replace('\\n', '').replace('ADVERTISEMENT','').replace('SCROLL TO RESUME CONTENT','')\n",
        "                # print(f'done[{a}] > {headline[0:40]}')\n",
        "                toReturn.append(content_)\n",
        "                a += 1\n",
        "    return toReturn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p933ewTRbebx"
      },
      "source": [
        "Function for Topic Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znXNLkZwbh1O"
      },
      "outputs": [],
      "source": [
        "import requests as req\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "def search_links(text, num_links):\n",
        "    query = 'penjelasan mengenai ' + text + ' dari sumber yang terpercaya'\n",
        "    search_url = 'https://www.google.com/search?q=' + query\n",
        "\n",
        "    # Send a GET request to the search URL\n",
        "    response = req.get(search_url)\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = bs(response.content, 'html.parser')\n",
        "\n",
        "    # Find the search results links\n",
        "    search_results = soup.find_all('a')\n",
        "\n",
        "    # Extract the URLs from the search results\n",
        "    links = []\n",
        "    for link in search_results:\n",
        "        url = link.get('href')\n",
        "        if url.startswith('/url?q='):\n",
        "            url = url[7:]  # Remove the '/url?q=' prefix\n",
        "            if '&' in url:\n",
        "                url = url[:url.index('&')]  # Remove additional parameters\n",
        "            links.append(url)\n",
        "\n",
        "    # Return the specified number of links or up to 5 links if available\n",
        "    return links[:min(num_links, 10)]\n",
        "\n",
        "\n",
        "\n",
        "    for keyword in keywords:\n",
        "        links = search_links(keyword, num_links=2)\n",
        "        link_recommendation.extend(links)\n",
        "\n",
        "    return link_recommendation\n",
        "\n",
        "def get_topic(text):\n",
        "    # Preprocessing and preparing the text data\n",
        "    news_data = [\n",
        "        text\n",
        "    ]\n",
        "\n",
        "    # Tokenize the document\n",
        "    tokenized_data = [news_data[0].split()]\n",
        "\n",
        "    # Create a dictionary from the tokenized data\n",
        "    dictionary = corpora.Dictionary(tokenized_data)\n",
        "\n",
        "    # Convert tokenized data into a bag-of-words representation\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in tokenized_data]\n",
        "\n",
        "    # Perform LDA\n",
        "    num_topics = 1  # Specify the number of topics\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "    # Print the topics and their keywords\n",
        "    num_words = 5  # Number of words to display per topic\n",
        "    for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=num_words):\n",
        "        print(f'Topic {idx + 1}: {topic}')\n",
        "        print(topic)\n",
        "\n",
        "    topic_keywords = topic\n",
        "\n",
        "    # Extracting keywords\n",
        "    keywords = [word.strip().split(\"*\")[1].replace(\"\\\"\", \"\") for word in topic_keywords.split(\"+\")]\n",
        "\n",
        "    link_recomendation = []\n",
        "\n",
        "    def mySearch(myItem):\n",
        "      query = 'penjelasan mengenai' + myItem + \"dari sumber yang terpercaya\"\n",
        "      for j in search_links(query, num_links=2):\n",
        "          link_recomendation.append(j)\n",
        "\n",
        "    for keyword in keywords:\n",
        "      mySearch(keyword)\n",
        "\n",
        "    return link_recomendation\n",
        "\n",
        "    # print(keywords)\n",
        "    # link_recommendation = []\n",
        "\n",
        "    # search_links(text, num_links=10)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# text = \"ibukota baru apakah di kalimantan?\"  # Ganti dengan teks yang ingin Anda analisis\n",
        "# links = search_links(text, num_links=10)\n",
        "# print(\"Recommended Links:\")\n",
        "# for link in links:\n",
        "#     print(link)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKa7mnYi67ra",
        "outputId": "4f338c75-1e39-427e-c85c-cdb4bb3effa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T88kHji-7KoM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvUwHJrJ7Umo"
      },
      "outputs": [],
      "source": [
        "max_length = 128  # Maximum length of input sentence to the model.\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "#mendefinisikan label yang ada , 0=entailment, 1 = netral, dan 2= kontradiksi\n",
        "# Labels in our dataset.\n",
        "labels = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "\n",
        "\n",
        "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"Generates batches of data.\n",
        "\n",
        "    Args:\n",
        "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
        "        labels: Array of labels.\n",
        "        batch_size: Integer batch size.\n",
        "        shuffle: boolean, whether to shuffle the data.\n",
        "        include_targets: boolean, whether to incude the labels.\n",
        "\n",
        "    Returns:\n",
        "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
        "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
        "         if `include_targets=False`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sentence_pairs,\n",
        "        labels,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        include_targets=True,\n",
        "    ):\n",
        "        self.sentence_pairs = sentence_pairs\n",
        "        self.labels = labels\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.include_targets = include_targets\n",
        "        # Load our BERT Tokenizer to encode the text.\n",
        "        # We will use base-base-uncased pretrained model.\n",
        "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "            \"indobenchmark/indobert-base-p2\", do_lower_case=True\n",
        "        )\n",
        "        self.indexes = np.arange(len(self.sentence_pairs))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __cleaning(self, text:str):\n",
        "        # clear punctuations\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # clear multiple spaces\n",
        "        text = re.sub(r'/s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def __len__(self):\n",
        "        # Denotes the number of batches per epoch.\n",
        "        return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieves the batch of index.\n",
        "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        sentence_pairs = self.sentence_pairs[indexes]\n",
        "\n",
        "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "        # encoded together and separated by [SEP] token.\n",
        "        encoded = self.tokenizer.batch_encode_plus(\n",
        "            sentence_pairs.tolist(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"tf\",\n",
        "            truncation = True,\n",
        "            truncation_strategy='only_first'\n",
        "        )\n",
        "\n",
        "        # Convert batch of encoded features to numpy array.\n",
        "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "        # Set to true if data generator is used for training/validation.\n",
        "        if self.include_targets:\n",
        "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "            return [input_ids, attention_masks, token_type_ids], labels\n",
        "        else:\n",
        "            return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
        "        if self.shuffle:\n",
        "            np.random.RandomState(42).shuffle(self.indexes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpWPRmj77-NT"
      },
      "source": [
        "Load model for similiarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9IcC0AJ8AFj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "loaded_model = tf.keras.models.load_model('./drive/MyDrive/Saved Models/model.h5', custom_objects={\"TFBertModel\": transformers.TFBertModel})\n",
        "# model = load_model('./drive/MyDrive/Saved Models/model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxcOGDFq78fe"
      },
      "outputs": [],
      "source": [
        "def check_similarity(sentence1, sentence2):\n",
        "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
        "    test_data = BertSemanticDataGenerator(\n",
        "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "    )\n",
        "\n",
        "    proba = loaded_model.predict(test_data[0])[0]\n",
        "    idx = np.argmax(proba)\n",
        "    final_proba = int(proba[idx] * 100)\n",
        "    proba = \"{:.0f}%\".format(final_proba)\n",
        "    pred = labels[idx]\n",
        "    # labels = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "    if(pred == 'entailment'):\n",
        "      return 'Berita yang anda masukan '+ proba +' memiliki kesamaan dengan sumber terpercaya'\n",
        "    elif(pred == 'contradiction'):\n",
        "      return 'Berita yang anda masukan '+ proba +' memiliki perbedaan dengan sumber terpercaya'\n",
        "    elif(pred == 'neutral'):\n",
        "      return 'Berita yang anda masukan '+ proba +' tidak memiliki keterkaitan dengan sumber terpercaya'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6SsbUmYbWza"
      },
      "source": [
        "Setup REST API with flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUT6ypy8FbFS",
        "outputId": "05735e8d-97d8-44d9-e12c-5271e8037299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://2a46-34-150-195-68.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "Topic 1: 0.026*\"di\" + 0.026*\"Indonesia\" + 0.019*\"konser\" + 0.019*\"Coldplay\" + 0.015*\"negara\"\n",
            "0.026*\"di\" + 0.026*\"Indonesia\" + 0.019*\"konser\" + 0.019*\"Coldplay\" + 0.015*\"negara\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [13/Jun/2023 13:00:18] \"POST /get-recomendation HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, request, jsonify\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/get-source-of-truth', methods=['POST'])\n",
        "def getSourceOfTruth():\n",
        "    data = request.form\n",
        "    title = data['title']\n",
        "\n",
        "    if title:\n",
        "        source_of_truth = scrape_detik(3, title)\n",
        "        print(source_of_truth)\n",
        "        return jsonify({'news_source_of_truth': source_of_truth})\n",
        "    else:\n",
        "        return jsonify({'error': 'Data not provided'})\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.form\n",
        "    source_of_truth = data['source_of_truth']\n",
        "    news_text = data['news_text']\n",
        "\n",
        "    if source_of_truth:\n",
        "        result_comparing = check_similarity(news_text, source_of_truth)\n",
        "        return jsonify({'predicition_result': result_comparing})\n",
        "    else:\n",
        "        return jsonify({'error': 'Data not provided'})\n",
        "\n",
        "@app.route('/get-recomendation', methods=['POST'])\n",
        "def getRecomendation():\n",
        "    data = request.form\n",
        "    news_text = data['news_text']\n",
        "\n",
        "    if news_text:\n",
        "        article_recomendation = get_topic(news_text)\n",
        "        return jsonify({'recomendations': article_recomendation})\n",
        "    else:\n",
        "        return jsonify({'error': 'Data not provided'})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}